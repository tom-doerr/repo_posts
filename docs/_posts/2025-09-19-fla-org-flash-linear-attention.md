---
layout: default
date: 2025-09-19T23:59:16.508378
image: assets/20250919T023737444--fla-org--flash-linear-attention--20250919T025233559--cropped.png
---

# [fla-org/flash-linear-attention](https://github.com/fla-org/flash-linear-attention)

Efficient Triton-based linear attention kernels for PyTorch and multiple hardware platforms
