---
layout: default
date: 2025-09-18T05:05:30.538805
image: assets/20250915T203301371--Dao-AILab--flash-attention--20250918T031309151--cropped.png
---

# [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)

fast attention algorithm for transformers, boosts speed and memory efficiency
